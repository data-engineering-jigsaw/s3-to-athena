{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee53ad89-35b6-4e6a-9438-521964ed132c",
   "metadata": {},
   "source": [
    "# Aws Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89e935-7111-4b1f-8fc8-451cde515379",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998226a-d749-4830-b17a-67561e730e75",
   "metadata": {},
   "source": [
    "In this lesson, we'll see how we can work with AWS athena.  Aws athena is a service that allows us to query our data directly from an s3 bucket.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b70af15-361a-4872-af91-486ae7cbc748",
   "metadata": {},
   "source": [
    "### Benefits of Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd166be3-141f-4299-bde8-bbaa75eba631",
   "metadata": {},
   "source": [
    "Athena will allow us to query our S3 data without setting up or running a database like our RDS postgres image.\n",
    "\n",
    "This has some benefits.  Remember with a postgres image in RDS, we have to be careful about keeping this database running, especially if we are not using it very often.  With athena, our data is stored in a file in S3.  Nothing is running until we call the query.\n",
    "\n",
    "Another benefit of athena is that we do not have to set up a traditional schema.  We can query unstructured data with athena.\n",
    "\n",
    "Still, athena has it's downsides.  It doesn't support typical database features like indexing, which is used to speed up our queries.  And with athena, we pay per query, so if running a lot of queries can have the cost add up.  With a service like RDS we pay per the size of the cluster, so the cost can be easier to predict. \n",
    "\n",
    "For these reasons, Athena is typically used for a couple adhoc queries, before moving the data to a database.  Or for some initial searching through unstructured data like log files.  You can read more about the pros and cons of Athena in the resources at the end of this lesson.  For now let's start using it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d1a8d-33b4-482f-b931-7829e410bb0c",
   "metadata": {},
   "source": [
    "### Using athena with S3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ed22c-f030-47c5-aa6a-b66c5c4ec3af",
   "metadata": {},
   "source": [
    "Now athena will allow us to query our Json or CSV data directly from S3.  \n",
    "\n",
    "To query JSON, our data needs to be in a specific form -- that looks like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c6c1e2-321e-47eb-94bc-1229ea84dc02",
   "metadata": {},
   "source": [
    "```python\n",
    "{\"song\": \"royals\", \"artist\": \"lorde\"}\n",
    "{\"song\": \"taxman\", \"artist\": \"the beatles\"}\n",
    "{\"song\": \"paint it black\", \"artist\": \"rolling stones\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f6406-a330-4f4f-b91f-5029a5ff4825",
   "metadata": {},
   "source": [
    "For our Json to work, each dictionary should be on a separate line in our s3 buckets, there should be no comma between our dictionaries, and we should not have any square brackets at the beginning or end of our list of dictionaries.\n",
    "\n",
    "For CSV data, we can just upload a standard csv file to s3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b0e3b-6397-4cea-822e-96811d78b781",
   "metadata": {},
   "source": [
    "### Storing our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dfc60b-b60e-4037-86a6-4b3cd38dd654",
   "metadata": {},
   "source": [
    "If you look at the `src/console.py` and `index.py` files, you can see how we accomplish this.  Looking at the `console.py` file:\n",
    "\n",
    "* We create a new bucket to store our data to query (you'll have to set a unique name).\n",
    "* We retrieve a list of dictionaries with a call to find_receipts. \n",
    "* We then use `pd.DataFrame` to convert this list of dictionaries to a dataframe.\n",
    "* Then if you uncomment the `s3.upload_file` method, you'll see that we add this csv file to our bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9e25c-ae10-4af9-994c-fd100ff32dd6",
   "metadata": {},
   "source": [
    "We can confirm that this works, by then reading from the bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1548ded5-22d1-4873-9919-e2819d60ac26",
   "metadata": {},
   "source": [
    "```python3\n",
    "bucket_name = ''\n",
    "object_name = ''\n",
    "obj = s3.get_object(Bucket=bucket, Key=object_name)\n",
    "text = obj['Body'].read()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877ee32d-6926-4025-aa06-45b8a1a37e68",
   "metadata": {},
   "source": [
    "Ok, so now that we have have a bucket and an object to read from, the next step is to create a bucket to write to.  It turns out that athena will be writing the results of our query to an object in a bucket, so let's create that too.  \n",
    "\n",
    "You can see in the `console.py` file, that we have a couple of lines for creating just this bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f66b43-f88e-4a28-bc38-0ad0db584181",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "\n",
    "bucket_name = 'jigsawtexasresults' # replace bucket name\n",
    "results_bucket = s3.create_bucket(Bucket = bucket_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c066ec-3eee-43b2-9001-646932787ea1",
   "metadata": {},
   "source": [
    "### Setting up Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ea677-6146-48ef-b620-5e52aa42e324",
   "metadata": {},
   "source": [
    "Ok, so we can confirm that our buckets have been created, and our file has been uploaded.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af0c830-6de0-41d1-b0d2-5b458b304fc3",
   "metadata": {},
   "source": [
    "From here we type athena in the search console, and click on athena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf7c8e-8e40-47d9-816a-a547557b34a4",
   "metadata": {},
   "source": [
    "<img src=\"./athena.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ece462-90c0-4eb3-8d7d-fc314e0ef21a",
   "metadata": {},
   "source": [
    "From there, click on Query your data, and click Launch query editor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acee4e-a6cc-47ac-a64c-a1edc37adbe8",
   "metadata": {},
   "source": [
    "<img src=\"./query-data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6606ecb4-fa5a-426a-af90-71c4d5d78361",
   "metadata": {},
   "source": [
    "When we get into Athena, we'll see in the light blue banner, that it asks us to set up an athena query results location.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116867a1-81d7-43de-9239-1c0c341ad26c",
   "metadata": {},
   "source": [
    "<img src=\"./query-result-location.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29a7959-05ef-4e8f-8b60-bfe3633d6b8b",
   "metadata": {},
   "source": [
    "Let's do that now.  This is just the path to the results bucket that we created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b95350-5abf-48a9-b7b4-88a33ac3c8fe",
   "metadata": {},
   "source": [
    "<img src=\"./results-bucket.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd477e8-c6e8-4133-a95e-4827f87a0921",
   "metadata": {},
   "source": [
    "So now that we have specified the results, the next step is to specify where we are getting our data from.  So to the left click on `create`, and then AWS Glue Crawler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ff688-d454-4d6c-b9e9-eacc5f2bc767",
   "metadata": {},
   "source": [
    "<img src=\"./aws-glue-crawler.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eca6ad-cf7f-46cf-9eea-57279a9d578f",
   "metadata": {},
   "source": [
    "By selecting AWS glue, we are instructing AWS to crawl the data in the specified s3 object, and then create a corresponding table from the attributes of our csv file.  \n",
    "\n",
    "So let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f94f3-6785-417a-8830-349f34ac7074",
   "metadata": {},
   "source": [
    "### Creating our Glue Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0cac3a-7245-4e4f-ae16-7baf82cd9563",
   "metadata": {},
   "source": [
    "Go to the new page, and follow the instructions of entering the crawler name, and then adding the data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634bcd3-59b0-4023-8bf8-c7c674909e3f",
   "metadata": {},
   "source": [
    "> You can see that for this step, we can specify the *bucket* where we uploaded our data to.  Notice that we placed a `/` at the end of the bucket name, indicating to crawl the files inside of the bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f9485-4cdd-4fc6-a1c9-e6452dab3cb0",
   "metadata": {},
   "source": [
    "<img src=\"./s3-bucket.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b494e-a63b-4375-b9e2-6b5f5a651022",
   "metadata": {},
   "source": [
    "After adding our S3 data source we should see something like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c569f116-c69a-4836-ad95-a1af621dc573",
   "metadata": {},
   "source": [
    "<img src=\"./selected-source.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1bd102-73d5-4639-b9ff-168faadd1f55",
   "metadata": {},
   "source": [
    "Next we will need a new iam role to read from our s3 bucket.  Click on `Create new IAM` role.  And you can view the default IAM configuration by clicking on `View`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48954f16-afe0-4076-9383-4a9ac25f9361",
   "metadata": {},
   "source": [
    "<img src=\"./iam-role.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b226d-2d7a-41a6-b021-22184a2f7335",
   "metadata": {},
   "source": [
    "<img src=\"./create_db.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac3d4d7-c930-4949-be04-112b3f8dcc5b",
   "metadata": {},
   "source": [
    "Click on add a database, and fill in a database name.  From there, if you click on the refresh button to the right, you will be able to see your database, and select it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc050b8-9c9e-4f41-bca2-663af89d01b1",
   "metadata": {},
   "source": [
    "<img src=\"./select-new-db.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d45b88-5075-4078-bb5e-e0ade646a236",
   "metadata": {},
   "source": [
    "> This database is created in something called the AWS Lake Formation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7507b1-d12e-4c70-b0c1-f3e2ea63f370",
   "metadata": {},
   "source": [
    "We can keep the database schedule as on demand.  However, AWS allows us to recrawl our buckets on a schedule in case the structure of our data changes.\n",
    "\n",
    "Finally, we can select create crawler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f47f28-9082-4e0a-9603-4ae35f9783c9",
   "metadata": {},
   "source": [
    "<img src=\"./create-crawler.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32aaed6-f216-4542-b8ee-c949a0f585da",
   "metadata": {},
   "source": [
    "If it worked, you should see a green banner saying that the crawler was created, and from there you can click on `Run crawler` to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b80d1c6-e708-4db1-8d86-741ea160bff1",
   "metadata": {},
   "source": [
    "<img src=\"./run-crawler.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf6a53-232d-4f10-9811-273176668f0a",
   "metadata": {},
   "source": [
    "### Back to Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a3e06-40a0-44ee-855e-3321783312f2",
   "metadata": {},
   "source": [
    "Ok, so remember that Glue just turned crawled our S3 bucket so that we could query this bucket as a table.  Now we can go back to Athena to perform some queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc67620-cf6e-4516-aaa4-1dc4577c3b3e",
   "metadata": {},
   "source": [
    "> For the database, select the database that we created in athena.  And then we can query our bucket as if it were a table.  \n",
    "\n",
    "So below, our query is:\n",
    "\n",
    "```sql\n",
    "select location_name, liquor_receipts from jigsawtexasquery where liquor_receipts = 0 limit 3;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d26060-3739-404d-98a8-028525a89984",
   "metadata": {},
   "source": [
    "<img src=\"./query-athena.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b4dbe-be5a-4756-92d0-ce53ea1ae52a",
   "metadata": {},
   "source": [
    "Finally, like everything, it is also possible to access use Athena from boto3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2475dc1-50fe-4133-81af-5176d3a4bd2a",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[AWS Athena](https://www.sqlshack.com/an-introduction-to-aws-athena/)\n",
    "\n",
    "[Athena pros and cons](https://towardsaws.com/aws-athena-why-is-it-different-than-mysql-93d55fd4a757)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee326931-1536-4fad-9dc2-23a50ba90834",
   "metadata": {},
   "source": [
    "[AWS permissions](https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e068d53-e240-4220-b10e-d5a2cc2649a2",
   "metadata": {},
   "source": [
    "[S3 permissions](https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8674864e-b68f-47d1-9bf3-f207b76eb9db",
   "metadata": {},
   "source": [
    "[boto bucket policy](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-bucket-policies.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
