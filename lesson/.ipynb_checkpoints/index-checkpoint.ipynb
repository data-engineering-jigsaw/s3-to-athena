{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee53ad89-35b6-4e6a-9438-521964ed132c",
   "metadata": {},
   "source": [
    "# Aws Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89e935-7111-4b1f-8fc8-451cde515379",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998226a-d749-4830-b17a-67561e730e75",
   "metadata": {},
   "source": [
    "In this lesson, we'll see how we can work with AWS athena.  Aws athena is a service that allows us to query our data directly from an s3 bucket.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b70af15-361a-4872-af91-486ae7cbc748",
   "metadata": {},
   "source": [
    "### Benefits of Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd166be3-141f-4299-bde8-bbaa75eba631",
   "metadata": {},
   "source": [
    "Athena will allow us to query our S3 data without setting up or running a database like our RDS postgres image.\n",
    "\n",
    "This has some benefits.  Remember that with a postgres image in RDS, we have to be careful about keeping this database running, especially if we are not using it very often.  Essentially, we'll be paying for idle time.  With athena, by contrast, our data is stored in a file in S3.  And nothing is running until we call the query.\n",
    "\n",
    "Another benefit of athena is that we do not have to set up a traditional schema.  So we can query unstructured data with athena.\n",
    "\n",
    "Still, athena has it's downsides.  It doesn't support typical database features like indexing, which is used to speed up our queries.  And with athena, we pay per query, so if running a lot of queries can have the cost add up.  \n",
    "\n",
    "For these reasons, Athena is typically used for a couple of adhoc queries, before moving the data to a database.  Or it's used for some initial searching through unstructured data like log files.  Ok, let's start using it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d1a8d-33b4-482f-b931-7829e410bb0c",
   "metadata": {},
   "source": [
    "### Using athena with S3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ed22c-f030-47c5-aa6a-b66c5c4ec3af",
   "metadata": {},
   "source": [
    "As we know Athena will allow us to query data directly from S3, and this data should be in Json or CSV form.  \n",
    "\n",
    "If we are storing JSON data, the data needs to be in a specific form that looks like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c6c1e2-321e-47eb-94bc-1229ea84dc02",
   "metadata": {},
   "source": [
    "```python\n",
    "{\"song\": \"royals\", \"artist\": \"lorde\"}\n",
    "{\"song\": \"taxman\", \"artist\": \"the beatles\"}\n",
    "{\"song\": \"paint it black\", \"artist\": \"rolling stones\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f6406-a330-4f4f-b91f-5029a5ff4825",
   "metadata": {},
   "source": [
    "So with Json, each dictionary should be on a separate line in our s3 buckets, there should be no comma between our dictionaries, and we should not have any square brackets at the beginning or end of our list of dictionaries.\n",
    "\n",
    "For CSV data, we can just upload a standard csv file to s3 (which is what we'll do)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b0e3b-6397-4cea-822e-96811d78b781",
   "metadata": {},
   "source": [
    "### Storing our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dfc60b-b60e-4037-86a6-4b3cd38dd654",
   "metadata": {},
   "source": [
    "If you look at the `src/console.py` and `index.py` files, you can see how we accomplish this.  Looking at the `console.py` file:\n",
    "\n",
    "* We create a new bucket to store our data to query (you'll have to set a unique name).\n",
    "* We retrieve a list of dictionaries with a call to `find_receipts`. \n",
    "* We then use `pd.DataFrame` to convert this list of dictionaries to a dataframe.\n",
    "* Then if you uncomment the `s3.upload_file` method, you'll see that we add this csv file to our bucket.\n",
    "\n",
    "> You'll have to specify a bucket name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9e25c-ae10-4af9-994c-fd100ff32dd6",
   "metadata": {},
   "source": [
    "We can confirm that this works, by then reading from the bucket like so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1548ded5-22d1-4873-9919-e2819d60ac26",
   "metadata": {},
   "source": [
    "```python\n",
    "bucket_name = ''\n",
    "object_name = ''\n",
    "obj = s3.get_object(Bucket=bucket, Key=object_name)\n",
    "text = obj['Body'].read()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877ee32d-6926-4025-aa06-45b8a1a37e68",
   "metadata": {},
   "source": [
    "Ok, so we've just created a bucket, and uploaded some CSV data for Athena to read our data from.\n",
    "\n",
    "The next step is to create a bucket to *write to*.  It turns out that athena will be writing the results of our query to an object in a bucket, so let's create this bucket to store the output of a query.  \n",
    "\n",
    "You can see in the `console.py` file, that we have a couple of lines for creating just this bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f66b43-f88e-4a28-bc38-0ad0db584181",
   "metadata": {},
   "source": [
    "```python\n",
    "bucket_name = 'jigsawtexasresults' # replace bucket name\n",
    "results_bucket = s3.create_bucket(Bucket = bucket_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9509b9-e19b-43b7-a487-f96665e6a3b9",
   "metadata": {},
   "source": [
    "Ok, so go to the s3 console, and confirm that our buckets have been created, and the query file has been uploaded.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c066ec-3eee-43b2-9001-646932787ea1",
   "metadata": {},
   "source": [
    "### Setting up Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af0c830-6de0-41d1-b0d2-5b458b304fc3",
   "metadata": {},
   "source": [
    "Ok, now that we set up our S3 buckets, it's time to work with Athena itself.  So type athena in the search console, and then click on athena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf7c8e-8e40-47d9-816a-a547557b34a4",
   "metadata": {},
   "source": [
    "<img src=\"./athena.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ece462-90c0-4eb3-8d7d-fc314e0ef21a",
   "metadata": {},
   "source": [
    "From there, click on Query your data, and click Launch query editor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acee4e-a6cc-47ac-a64c-a1edc37adbe8",
   "metadata": {},
   "source": [
    "<img src=\"./query-data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6606ecb4-fa5a-426a-af90-71c4d5d78361",
   "metadata": {},
   "source": [
    "When we get into Athena, we'll see in the light blue banner at the top that `Before you run your first query, you need to set up a query result location in Amazon S3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116867a1-81d7-43de-9239-1c0c341ad26c",
   "metadata": {},
   "source": [
    "<img src=\"./query-result-location.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29a7959-05ef-4e8f-8b60-bfe3633d6b8b",
   "metadata": {},
   "source": [
    "Let's do that now.  This is just the path to the results bucket that we created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b95350-5abf-48a9-b7b4-88a33ac3c8fe",
   "metadata": {},
   "source": [
    "<img src=\"./results-bucket.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd477e8-c6e8-4133-a95e-4827f87a0921",
   "metadata": {},
   "source": [
    "So now that we have specified where Athena will place the results, the next step is to specify where we are getting our data from.  To do so, to the left, click on `Create`, and then AWS Glue Crawler as the source of the data that we will get our data from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ff688-d454-4d6c-b9e9-eacc5f2bc767",
   "metadata": {},
   "source": [
    "<img src=\"./aws-glue-crawler.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eca6ad-cf7f-46cf-9eea-57279a9d578f",
   "metadata": {},
   "source": [
    "By selecting AWS glue, we are instructing AWS to crawl the data in the specified s3 object, and then create a corresponding table from the attributes of our csv file.  \n",
    "\n",
    "So let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f94f3-6785-417a-8830-349f34ac7074",
   "metadata": {},
   "source": [
    "### Creating our Glue Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0cac3a-7245-4e4f-ae16-7baf82cd9563",
   "metadata": {},
   "source": [
    "Go to the new page that popped up when clicking on AWS Glue Crawler, and follow the instructions of entering the crawler name, and then adding the data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634bcd3-59b0-4023-8bf8-c7c674909e3f",
   "metadata": {},
   "source": [
    "> You can see that for this step of adding a data source, we can specify the **bucket** where we uploaded our data to.  Notice that we placed a `/` at the end of the bucket name, indicating to crawl the objects inside of the bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f9485-4cdd-4fc6-a1c9-e6452dab3cb0",
   "metadata": {},
   "source": [
    "<img src=\"./s3-bucket.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b494e-a63b-4375-b9e2-6b5f5a651022",
   "metadata": {},
   "source": [
    "After adding our S3 data source we should see something like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c569f116-c69a-4836-ad95-a1af621dc573",
   "metadata": {},
   "source": [
    "<img src=\"./selected-source.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1bd102-73d5-4639-b9ff-168faadd1f55",
   "metadata": {},
   "source": [
    "Next we will need a new iam role to read from our s3 bucket.  Click on `Create new IAM` role.  And you can view the default IAM configuration created for you by clicking on `View`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48954f16-afe0-4076-9383-4a9ac25f9361",
   "metadata": {},
   "source": [
    "<img src=\"./iam-role.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d7f07-acf0-4e98-bbc8-19d85484b947",
   "metadata": {},
   "source": [
    "There, if you expand the S3 policy, this should make some sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad634920-1870-46c8-bc9b-4d2b4bd4a28e",
   "metadata": {},
   "source": [
    "<img src=\"./s3-access.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e737f8f0-fe11-4593-a31e-d8b05f977bc5",
   "metadata": {},
   "source": [
    "We can see that for our query bucket, we are granting Glue GetObject and PutObject access on our specified query bucket.\n",
    "\n",
    "Ok, next is to choose a Target database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b226d-2d7a-41a6-b021-22184a2f7335",
   "metadata": {},
   "source": [
    "<img src=\"./create_db.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac3d4d7-c930-4949-be04-112b3f8dcc5b",
   "metadata": {},
   "source": [
    "Click on add a database, and fill in a database name.  From there, if you click on the refresh button to the right, you will be able to see your database, and select it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc050b8-9c9e-4f41-bca2-663af89d01b1",
   "metadata": {},
   "source": [
    "<img src=\"./select-new-db.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d45b88-5075-4078-bb5e-e0ade646a236",
   "metadata": {},
   "source": [
    "> This database is created in something called the AWS Lake Formation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7507b1-d12e-4c70-b0c1-f3e2ea63f370",
   "metadata": {},
   "source": [
    "We can keep the database schedule as on demand.  Glue has the ability to recrawl our buckets on a schedule in case the structure of our data changes.\n",
    "\n",
    "> Keep the schedule as `on demand`.\n",
    "\n",
    "Finally, we can select create crawler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f47f28-9082-4e0a-9603-4ae35f9783c9",
   "metadata": {},
   "source": [
    "<img src=\"./create-crawler.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32aaed6-f216-4542-b8ee-c949a0f585da",
   "metadata": {},
   "source": [
    "If it worked, you should see a green banner saying that the crawler was created, and from there you can click on `Run crawler` to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b80d1c6-e708-4db1-8d86-741ea160bff1",
   "metadata": {},
   "source": [
    "<img src=\"./run-crawler.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758a352-e675-4ec8-a2a5-3dc91fa2e3e7",
   "metadata": {},
   "source": [
    "> If you see an error, one option is to type in lake formation, and make sure that there is admin access (when prompted).  Another way to troubleshoot is to look through the list of IAM users and make sure that none of the access keys are compromised.  If they are, you should reset them or delete the account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf6a53-232d-4f10-9811-273176668f0a",
   "metadata": {},
   "source": [
    "### Back to Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a3e06-40a0-44ee-855e-3321783312f2",
   "metadata": {},
   "source": [
    "Ok, so remember that Glue just turned crawled our S3 bucket so that we could query this bucket as a table.  Now we can go back to Athena to perform some queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc67620-cf6e-4516-aaa4-1dc4577c3b3e",
   "metadata": {},
   "source": [
    "> For the database, select the database that we created in athena.  And then we can query our bucket as if it were a table.  \n",
    "\n",
    "So below, our query is:\n",
    "\n",
    "```sql\n",
    "select location_name, liquor_receipts from jigsawtexasquery where liquor_receipts = 0 limit 3;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d26060-3739-404d-98a8-028525a89984",
   "metadata": {},
   "source": [
    "<img src=\"./query-athena.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b4dbe-be5a-4756-92d0-ce53ea1ae52a",
   "metadata": {},
   "source": [
    "Finally, like everything, it is also possible to access use Athena from boto3.  You can take a look at that in the `src/athena_boto.py` file, and we can talk through it in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2475dc1-50fe-4133-81af-5176d3a4bd2a",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[AWS glue](https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html)\n",
    "\n",
    "[AWS Athena](https://www.sqlshack.com/an-introduction-to-aws-athena/)\n",
    "\n",
    "[Athena pros and cons](https://towardsaws.com/aws-athena-why-is-it-different-than-mysql-93d55fd4a757)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
